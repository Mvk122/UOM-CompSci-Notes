> These answers can be wrong as I wrote them like I would in a real exam, without reference to the notes
## Question 1a
1. The load instruction is issued, the operands are read, including the virtual address and the instruction is executed
2. The processor then sends the virtual address to be read to the MMU and to its L1 cache.
3. It is assumed that the L1 cache uses virtual addresses, since the page has not previously been accessed, it is assumed that this is a cache miss hence the data is not returned yet. If it was a cache hit, the result would be returned to the processor.
4. In the MMU, the virtual address is sent to the TLB to translate it to a physical address.
5. Since the page has not been used before, it can be assumed that the TLB also misses hence the page table need to be walked to convert it to a physical address.
6. This physical address is then sent to the L2, main memory and write buffer in parallel to return the data.
7. Since this page has not been accessed before, it is likely that it is not in the L2 cache hence there is a cache miss and the data needs to be returned by either the main memory or write buffer.
8. If the memory location was recently written to, it is probable that the contents of the write were in the write buffer and thus it can return the data and send it to the processor, cancelling the main memory operation.
9. If it is not in the write buffer, then finally the data has to be returned by the main memory.

## Question 1b
1. The data is written to the cache and it is marked as dirty. (assuming it is in the cache still, if it is not then a memory fetch is done first to populate the cache line). This is assuming the method of writing straight to main memory is not chosen on write misses.
2. Thus the cache and main memory are out of sync and the most up to date value is in the cache.
3. In a multicore processor which uses the MESI protocol to communicate cache synchronisation with cores that have their own L1 caches, if this cache entry was exclusive, nothing is done, if shared, an invalidate is sent to tell the other cores to invalidate their copy of this data in the L1 cache. 
4. When this value in cache gets evicted, only then write this value to memory.

## Question 2a
* It speeds up write operations by allowing the CPU to not have to wait for the confirmation when a write is sent to memory as writing to memory is slow.

## Question 2b
* The CPU needs to wait for read operations to succeed before proceeding.
* Hence if a variable was written to main memory in a recent previous instruction, the CPU can read the value from the write buffer instead of the main memory.
* This prevents it from having to wait for the previous instruction's write to fully complete. 
* Furthermore latency could be decreased as the write buffer could be closer and faster than main memory.

## 2c
* A memory barrier is created if memory mapped I/O is being used, preventing successive operations from proceeding before the barrier passes.
* This is because memory mapped I/O can return write acknowledgements wherein if a write is in the buffer and a read is done on the memory mapped I/O, a write acknowledgement for a previous write could be returned instead.
* This problem can be resolved by marking memory locations as disallowing buffering, hence writes to these addresses must be completed before the processor can continue operation.

## 2d
* No idea

## 2e
* same

## 3
1 LDR R2, A  
2 LDR R3, B  
3 SUB R1, R2, R3  
4 LDR R4, C  
5 MUL R7, R4, R4  
6 MUL R8, R7, R1  
7  STR R8, Z

## 3i and ii
* RAW Hazards: 
	* 1,3
	* 3,6
	* 4,5
	* 5,6
	* 7,6
* 17 cycles, 7 instructions so 2.428 cycles per instructions
* Forwarding could be used to pass the values to instructions that need them before the WB stage of the previous instruction.

## 3bi
1 LDR.D F0, A  
2 LDR.D F1, B  
3 ADD.D F4, F0, F7  
4 MUL.D F2, F0, F3  
5 STR.D F4, X  
6 SUB.D F4, F1, F2  
7 DIV.D F2, F0, F5  
8 MUL.D F1, F2, F7
* RAW:
	* 1,7
	* 1,3
	* 1,4
	* 3,5
	* 7,8
* WAR: 
	* 4,7
	* 6,5
* WAW:
	* 2,8
	* 3,6

## 3bii
 