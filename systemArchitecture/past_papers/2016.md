## 1a
* Compulsory misses: When the cache starts out cold and the cache has not been filled yet and the data does not exist in the cache.
* Capacity misses: The data item was replaced to make space in the cache
* Conflict misses: The cache architecture limits flexibility in the locations that a memory address can be located.

## 1b
* Cache size:
	* Compulsory misses: No change
	* Capacity misses: Decreases as cache size increases
	* Conflict: Decreases 
* Line size increase:
	* compulsory: reduce
	* Capacity: no change
	* conflict: increase
* associativity increase: 
	* conflict: decrease

## 1c
* Spatial locality: If a memory location was used, adjacent memory locations are likely to also be used soon. This can be seen when looping over contiguous arrays or in the stack.
* Temporal locality: If a memory location was used, it is likely to be used again. Eg when using a counter.

## 1d
1. $time = t_h + (t_m * (1-h))$
5. Adding an L2 cache, speeding up the memory/ L1 cache, increasing the size/associativity of the L1 cache to improve hit rate. Parallel access when accessing L1 and memory.

## 2e
1. Increased reliability by using parity bits/ blocks such that if one or more hard drives in the raid configuration fail, the data can be rebuilt using the parity bits. RAID1
2. Increasing read/write throughput by striping bits across the drives. RAID0 

## 3a
* Pipelining refers to breaking up instruction execution into different stages such that multiple instructions can overlap in their execution. 
* This can generally lead to a speed up in the execution. and better utilisation of the hardware
* Data hazards are an issue wherein later instructions require values from the register bank that have not been written to by previous instructions that are still in progress. 
* Hence WAW, WAR and RAW data hazards exist. 
* Furthermore, programs don't run all their instructions in sequence due to branching hence if a branch is wrongly predicted, there is a performance penalty for rolling back. (control hazards)

## 3b
1 MUL R2, R0, R1  
2 MUL R3, R1, R1  
3 MUL R4, R0, R0  
4 SUB R5, R1, R0  
5 CMP R5, #0  
6 ADD R6, R4, R3  
7 ADD R7, R6, R2  
8 DIV R8, R7, R5

## 3c
* It is good to run on a 2 way superscalar processor as 2 instructions can be run at the same time.

## 3d
* Compiler pros:
	* Can be run on simpler hardware
	* Can yield better performance as the window of instructions is larger and the higher level language used by the compiler can yield more meaning.
* Compiler cons:
	* Can be bloated with NOPs when using VLIW instructions
	* Limits executable to have the best performance for one architecture and configuration
* Hardware pros: 
	* Can gain performance by having the context of the hardware at runtime.
	* Binary can be more portable as a generalisable executable can be run on many hardware configurations
* Hardware cons: 
	* Requires more complex hardware that has:
		* Instruction buffers
		* Dynamic Scheduler
		* Data buffers and forwarding

## 3e
* A dynamic instruction scheduler that can detect dependencies and dispatch instructions using either a scoreboard or reservation stations 
* Results management that forwards results, prevents data dependencies and stalls/ renames registers.

## 4a
* Cache thrashing occurs when 2 threads are using memory locations that occupy the same location in the cache, leading to excessive cache misses on context switches. Or in programs which use 2 memory locations that are in the same cache location.
* False sharing refers to when 2 unrelated memory locations used by multiple cores exist within the same cache line, generating a lot of invalidate/update traffic.

## 4b
* Superscalar: Replication of execution logic to allow for issuing and execution of several instructions per cycle in a single thread.
* Multithreading: Control and status logic are replicated so that instructions from several flows can be issued at once, this allows for more ILP as different threads are likely to not have data dependencies between them. Execution logic is shared.
* Multicore: All processing logic is replicated so different instruction flows are executed independently

## 4c
1. Config 2:
	1. 6 GInstructions
	2. 10 GInstructions 
	3. 6 GInstructions
2. Config:
	1. 32
	2. 24
	3. 16
3. Config 3:
	1. 48
	2. 60
	3. 48
4. Config 2:
	1. 8
	2. 24
	3. 16

## 4d
* Changes in data stored in the cache of one core is seen in other cores with copies of the same data.