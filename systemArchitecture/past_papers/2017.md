## 1a
* Option 1: $(1*0.99 + 14*0.01) + (0.5)(1*0.9 + 14*0.1) =2.28$
* Option 2: $(0.02*14 + 0.98)*1.5 = 1.89$
* Option 2 is better by 20.6%

## 1b
* Offset: 5
* Index: 5
* Tag: 6

## 1c
1. Tag: 0101 11, index: 11 101 miss
2. all miss except #4

## 1d
1. Compulsory miss
2. Compulsory miss
3. Compulsory miss
4. Hit
5. Conflict miss

## 1e
* 50% speed up = done in 2 cycles
* 40%

## 3a
* Data hazards occur in superscalar or out of order processors when an out of order execution requires data that is written by a previous instruction to a register that has not been written to the register bank yet. This can lead to a pipeline stall. 
* Forwarding can be used to pass the intermediate results of computations eg in between the EX and EX stage or MEM and EX stage to previous stages to allow future instructions to use the values before they have been written to the register bank. Alternatively, buffering can be used to store the previous value.
* Control hazards occur in superscalar processors wherein the next instruction to be executed is not necessarily the one in sequential order from the previous one due to a conditional or unconditional branch instruction. This becomes an issue when: 
	* We only know its a branch instruction in the decode stage, which is too late as it occurs in parallel with the instruction fetch phase.
	* We don't know if the branch should be taken because it is a conditional branch and this is only discovered in the EX stage.
* A branch target buffer can be used to store the branch targets of previously ran instructions such that if it is encountered again, we can skip to the target of the branch instead. This would greatly reduce the penalties if the branches were used in loops and looped over many times. Otherwise it could be a detriment as incorrectly predicting a branch incurs expensive rollback penalties.

## 3b
* Memory wall:
	* Memory has not gained speed as fast as processors have, hence a lot of time is spent waiting on memory.
* Power wall: 
	* Increasing miniaturisation and clock speeds means that more heat needs to be dissipated from a smaller area which can be unfeasible
* Quantum effects:
	* Increasing miniaturisation leads to unreliable transistors due to quantum tunnelling at small scales. 
* Architectural complexity:
	* It is harder to innovate as the complexity of the chips are too hard to understand.
* Instruction Level parallelism:
	* There is a limit to the implicit parallelism between instructions in 1 thread and there are many hardware costs when it comes to scaling up hardware to aid parallelism like increasing the amount of instructions that can be fetched and the look-ahead window size.

## 3c
* Compiler pros:
	* Can yield better results as static analysis can be more detailed and with a larger window of instructions, furthermore it is easier for the compiler to understand the meaning of programs.
	* Can allow for simpler hardware.
* Compiler cons: 
	* Restricts compiled binaries to specific hardware architectures and configurations.
	* Binaries can become bloated in VLIW with excessive NOPs
* Hardware cons:
	* Requires more hardware like dynamic schedulers, instruction buffers and deferred memory access.
* Hardware pros:
	* Compatibility with more binaries 
	* Allowances for simpler compilers
	* Adaptability

## 4a
1. Will allow for the CPU design to be much simpler but will be very slow and not be able to exploit the parallelism implicit in the design of the software.
2. Will allow for faster processing at the cost of a more complex CPU however it isn't the best due to low ILP.
3. Will allow a multiple times speed up due to the parallelism in the design at the cost of a more complex CPU and will have much faster memory accesses as the threads can share data in the cache between the 4 operations. This will allow the low ILP threads to increase utilisation of the resources by reducing stalls.
4. Will allow for even more speed up than multicore however, given that the frame fits entirely into the cache, if each core has its own L1 cache it could lead to thrashing as multiple cores try to use the same cache and maintain cache coherency.
5. Multithreading is the best solution in terms of utilisation and performance.

## 4b
* Memory consistency: When the order of operations on memory appears to be sequential.
* Memory coherence: When changes in the memory are reflected across all caches to prevent outdated copies.

## 4c
* Lock: Only 1 thread is able to access a critical section of the code at once.
* Barrier: All instructions have completed before crossing the barrier.
* Fence: All instructions before the fence must complete before instructions after.